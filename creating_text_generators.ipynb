{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "creating_text_generators.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# RNN ile Metin Üretimi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "Bu dosya karakter temelli tahmin işlemi gerçekleştiren RNN modeli ile metin üretimine dair bir rehber niteliğindedir. \n",
        "\n",
        "Not: Dosyayı daha hızlı çalıştırabilmek için GPU Hızlandırıcısını açabilirsiniz. *Çalışma zamanı > Çalışma zamanı türünü değiştir > Donanım hızlandırıcı > GPU*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "Veri setlerinde yer alan cümlelerin bazıları kurallı ve tamamlanmış iken bazıları yarım veya devrik olabilmektedir. Model kelimelerin anlamını öğrenmemektedir ancak şunları göz önünde bulundurmak faydalı olacaktır:\n",
        "\n",
        "* Model karakter temelli çalışmakta. Eğitim başladığında model, kelimeleri nasıl heceleyeceğini veya bu kelimelerin bir metnin parçası olup olmadığını bile bilmemekte.\n",
        "\n",
        "* Model, küçük (her biri 100 karakter) metin grupları (batch) ile eğitilmiş olsa da tutarlı bir yapıya sahip uzun metinler oluşturabilmekte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Kurulum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Tensorflow'un ve Diğer Kütüphanelerin Import Edilmesi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG_n40gFzf9s",
        "outputId": "8623dbe5-5ce8-4237-b818-63c82644f760"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import urllib\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Veri Setinin İndirilmesi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/ardauzunoglu/sanatkar.ai/main/data-sets/tirad_veri_setleri/kadin_tirad_veri_setleri/kadin_tirad_veri_seti.txt\"\n",
        "\n",
        "data_file = urllib.request.urlopen(url)\n",
        "data = \"\"\n",
        "\n",
        "for line in data_file:\n",
        "  line = line.decode(\"utf-8\")\n",
        "  data += line\n",
        "  data += \"\\n\"\n",
        "\n",
        "text = data\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Verinin Okunması\n",
        "\n",
        "Metne göz atalım:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "21101bbb-d65f-43da-ada4-65460381275f"
      },
      "source": [
        "#Metindeki ilk 250 karaktere bakalım.\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aman yarabbim... Karar vermek ne güç şeymiş... Bir kişi, iki kişi olsa ne ise... Ama dört kişi... Gel de birini seç. Nikanor İvanoviç biraz zayıf ama hiç de fena değil. İvan Kuzmiç de fena değil. Açık konuşmak gerekirse, İvan Pavloviç de biraz şişman\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "926d5cfc-f458-4932-d42c-6b2f97854872"
      },
      "source": [
        "#Metindeki eşsiz karakter sayısı.\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} eşsiz karakter')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Metnin İşlenmesi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Metnin Vektörize Edilmesi\n",
        "\n",
        "Eğitime başlamadan önce karakter tabanlı değerleri (string) sayısal bir gösterime dönüştürmeliyiz. \n",
        "\n",
        "`preprocessing.StringLookup` katmanı her karakteri sayısal bir kimliğe dönüştürebilir, ancak bunun için önce metnin belirteçlere bölünmesi gerekiyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a86OoYtO01go",
        "outputId": "658d725b-7cea-4ce5-fb5e-8e490f82471e"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "`preprocessing.StringLookup` katmanını oluşturalım:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "Bu katman her bir tokeni karakter kimliğine dönüştürmektedir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLv5Q_2TC2pc",
        "outputId": "49e42127-7aaf-4964-8960-a20e79e6728d"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[47, 48, 49, 50, 51, 52, 53], [0, 69, 70]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Not: Bu aşamada `sorted(set(text))` ile üretilen kelime listesini kullanmak yerine `preprocessing.StringLookup` katmanının `get_vocabulary()` fonksiyonunu kullanıyoruz ki `[UNK]`* tokenleri de aynı şekilde ayarlanabilsin.\n",
        "\n",
        "*`[UNK]`, tokenize etme işlemi sırasında oluşmuş bilinmeyen kelimelerin yerini tutan bir değerdir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "Bu katman, karakterleri kimlik vektörlerinden kurtarır ve onları `tf.RaggedTensor` karakterleri olarak döndürür:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2GCh0ySD44s",
        "outputId": "cb999f23-1f57-48d0-f2d0-a816a408f17d"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'[UNK]', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "Karakterleri tekrar karakter tabanlı değerlere çevirmek için `tf.strings.reduce_join` kullanılabilir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxYI-PeltqKP",
        "outputId": "b5c99fb3-c553-4c96-bf2d-593c8ce333a9"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'[UNK]yz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### Tahmin Görevi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Bir karakter veya bir karakter dizisi verildiğinde, onları takip edecek en olası karakter nedir?  Bu soru, modeli gerçekleştirmesi için eğittimiz görevdir. Modele verilecek girdi bir karakter dizisi olacaktır ve model her time step'te bir sonraki karakteri tahmin edecek şekilde çıktıyı oluşturacaktır.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Eğitim Örneklerinin ve Hedeflerin Oluşturulması\n",
        "\n",
        "Metni her bir giriş dizisi `seq_length` içerecek şekilde örnek dizilere bölüyoruz.\n",
        "\n",
        "Her giriş dizisi için, bir karakter sağa kaydırılanlar dışında, karşılık gelen hedefler aynı uzunlukta metin içerir.\n",
        "\n",
        "Bu yüzden metni 'seq_length+1' parçalarına ayırıyoruz. Örneğin, \"seq_length\" 4 karakter uzunluğunda ve metnimiz \"Naber\" diyelim. Giriş dizisi \"Nabe\" ve hedef dizi \"aber\" olacaktır.\n",
        "\n",
        "Bunu yapabilmek adına metin vektörünü bir karakter indeksleri akışına dönüştürmek için `tf.data.Dataset.from_tensor_slices` fonksiyonunu kullanıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UopbsKi88tm5",
        "outputId": "c7741663-8868-42a0-f745-91e4e65700fd"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(86068,), dtype=int64, numpy=array([22, 59, 47, ..., 47, 10,  1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjH5v45-yqqH",
        "outputId": "4f5fdc58-3b0e-4e29-a928-f9e134052811"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "m\n",
            "a\n",
            "n\n",
            " \n",
            "y\n",
            "a\n",
            "r\n",
            "a\n",
            "b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "`batch`, bu tekil karakterleri istediğimiz boyuttaki dizilere kolayca dönüştürmemize olanak tanımaktadır."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpdjRO2CzOfZ",
        "outputId": "f2dd8c65-3c77-45f7-ed32-e6af41e227f4"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'A' b'm' b'a' b'n' b' ' b'y' b'a' b'r' b'a' b'b' b'b' b'i' b'm' b'.'\n",
            " b'.' b'.' b' ' b'K' b'a' b'r' b'a' b'r' b' ' b'v' b'e' b'r' b'm' b'e'\n",
            " b'k' b' ' b'n' b'e' b' ' b'g' b'\\xc3\\xbc' b'\\xc3\\xa7' b' ' b'\\xc5\\x9f'\n",
            " b'e' b'y' b'm' b'i' b'\\xc5\\x9f' b'.' b'.' b'.' b' ' b'B' b'i' b'r' b' '\n",
            " b'k' b'i' b'\\xc5\\x9f' b'i' b',' b' ' b'i' b'k' b'i' b' ' b'k' b'i'\n",
            " b'\\xc5\\x9f' b'i' b' ' b'o' b'l' b's' b'a' b' ' b'n' b'e' b' ' b'i' b's'\n",
            " b'e' b'.' b'.' b'.' b' ' b'A' b'm' b'a' b' ' b'd' b'\\xc3\\xb6' b'r' b't'\n",
            " b' ' b'k' b'i' b'\\xc5\\x9f' b'i' b'.' b'.' b'.' b' ' b'G' b'e' b'l'], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "Bu işlemi daha rahat anlamak için tokenleri tekrar karakter tabanlı değerlere dönüştürebiliriz:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO32cMWu4a06",
        "outputId": "da9ee3c2-242d-4b62-cee4-909cdaaa801b"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Aman yarabbim... Karar vermek ne g\\xc3\\xbc\\xc3\\xa7 \\xc5\\x9feymi\\xc5\\x9f... Bir ki\\xc5\\x9fi, iki ki\\xc5\\x9fi olsa ne ise... Ama d\\xc3\\xb6rt ki\\xc5\\x9fi... Gel'\n",
            "b' de birini se\\xc3\\xa7. Nikanor \\xc4\\xb0vanovi\\xc3\\xa7 biraz zay\\xc4\\xb1f ama hi\\xc3\\xa7 de fena de\\xc4\\x9fil. \\xc4\\xb0van Kuzmi\\xc3\\xa7 de fena de\\xc4\\x9fil. A\\xc3\\xa7\\xc4\\xb1k k'\n",
            "b'onu\\xc5\\x9fmak gerekirse, \\xc4\\xb0van Pavlovi\\xc3\\xa7 de biraz \\xc5\\x9fi\\xc5\\x9fman ama, pekala g\\xc3\\xb6steri\\xc5\\x9fli bir erkek. S\\xc3\\xb6yleyin bana ne y'\n",
            "b'apay\\xc4\\xb1m? Baltazar Baltazarovi\\xc3\\xa7 de de\\xc4\\x9ferli bir adam. Ah ne zor \\xc5\\x9fey bu karar vermek... Anlatamam, anlata'\n",
            "b\"mam. Nikonor \\xc4\\xb0vanovi\\xc3\\xa7'in dudaklar\\xc4\\xb1n\\xc4\\xb1, \\xc4\\xb0van Kuzmi\\xc3\\xa7'in burnunu alsak... Baltazar Baltazarovi\\xc3\\xa7'in de hal\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "Eğitim için birer dizi olan girdi ve etiket değerlerinden oluşan `(girdi, etiket)` çiftlerini içeren bir veri setine ihtiyacımız var.\n",
        "\n",
        "Girdi olarak bir dizi alan ve her time step için girdiyi ve etiketi hizalayan bir fonksiyon tanımlayabiliriz:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxbDTJTw5u_P",
        "outputId": "f47eef42-1cf5-4017-b601-ef52a296d61d"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbw-iR0ymwj",
        "outputId": "5806e5d9-5e79-496d-fb74-3f18bd1417b8"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'Aman yarabbim... Karar vermek ne g\\xc3\\xbc\\xc3\\xa7 \\xc5\\x9feymi\\xc5\\x9f... Bir ki\\xc5\\x9fi, iki ki\\xc5\\x9fi olsa ne ise... Ama d\\xc3\\xb6rt ki\\xc5\\x9fi... Ge'\n",
            "Target: b'man yarabbim... Karar vermek ne g\\xc3\\xbc\\xc3\\xa7 \\xc5\\x9feymi\\xc5\\x9f... Bir ki\\xc5\\x9fi, iki ki\\xc5\\x9fi olsa ne ise... Ama d\\xc3\\xb6rt ki\\xc5\\x9fi... Gel'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Eğitim için Batch Oluşturmak\n",
        "\n",
        "Metni daha kolay kullanılabilir dizilere bölmek için `tf.data` kullandık. Ancak bu verileri modele beslemeden önce verileri karıştırmamız ve yığınlar halinde paketlememiz gerekir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2pGotuNzf-S",
        "outputId": "8fa00050-8dc7-43be-89d1-1b379c3e79d0"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "#Veri setini karmak için kullanılan buffer boyutu\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Modeli Geliştirmek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "Bu kısım modeli bir `keras.Model` alt sınıfı olarak tanımlamaktadır.\n",
        "\n",
        "Bu model üç katmana sahiptir:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: Girdi katmanı. Her karakter kimliğini \"embedding_dim\" boyutlarına sahip bir vektörle eşleyecek, eğitilebilir bir arama tablosu.\n",
        "\n",
        "* `tf.keras.layers.GRU`: `units=rnn_units` boyutuna sahip bir RNN türü (Burada alternatif olarak LSTM katmanı da kullanabilir.)\n",
        "\n",
        "* `tf.keras.layers.Dense`: Çıktı katmanı. Sözlükteki her karakter için bir logit çıktısı verir. Bu çıktılar, modele göre her karakterin log-olasılığıdır."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "#Kelime listesi uzunluğu\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "#Embedding boyutu\n",
        "embedding_dim = 256\n",
        "\n",
        "#RNN ünitelerinin sayısı\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "Model, her bir karakter için embedding'i arar, girdi olarak embedding'i kullanarak bir kez GRU'yu çalıştırır ve bir sonraki karakterin log-olasılığını tahmin eden logitler oluşturmak için dense katmanı uygular:\n",
        "\n",
        "![](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Modeli Denemek\n",
        "\n",
        "Şimdi beklediğimiz gibi davranıp davranmadığını görmek için modeli çalıştıralım.\n",
        "\n",
        "İlk önce çıktının şeklini kontrol edelim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-_70kKAPrPU",
        "outputId": "87730b62-94e7-4817-d8e0-3f4584433863"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 91) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "Yukarıdaki örnekte, girdinin dizi uzunluğu `100`'dür, ancak model herhangi bir uzunluktaki girdiler üzerinde çalıştırılabilir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "281a209f-2156-4365-8009-9a4f6e8af1c8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      multiple                  23296     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  93275     \n",
            "=================================================================\n",
            "Total params: 4,054,875\n",
            "Trainable params: 4,054,875\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "Modelden gerçek tahminler almak için, gerçek karakter indekslerini elde etmek için çıktı dağılımından örneklememiz gerekir. Bu dağılım, karakter sözlüğü üzerindeki logitlerle tanımlanır.\n",
        "\n",
        "Not: Bu dağıtımdan _örnek_ almak önemlidir, çünkü dağıtımın _argmax_ değerini almak modeli bir döngüde kolayca sıkıştırabilir.\n",
        "\n",
        "Batch'deki ilk örnek için deneyelim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "Bu bize her time stepte bir sonraki karakter indeksinin bir tahminini verir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Eğitimsiz model tarafından tahmin edilen metni görmek için bunları decode edelim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWcFwPwLSo05",
        "outputId": "00e17985-e21d-4b88-a07a-300b5addfcf3"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'\\xc4\\xb1n yadigar\\xc4\\xb1. Etimi yese de kemiklerimi saklar... \\xc4\\xb0nsan ya\\xc5\\x9fad\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1 yere benzer... \\xc5\\x9eu gen\\xc3\\xa7 ya\\xc5\\x9f\\xc4\\xb1nda y\\xc3\\xbcz\\xc3\\xbcn'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"tseNzZdm'\\xc3\\xbcr1L\\xe2\\x80\\xa6z;nGV\\xc3\\xa2f9I\\xc5\\x9e))\\xc4\\x9fFuPnCu\\xc3\\x9c'r7\\xc3\\xa7\\xe2\\x80\\x9c\\xc4\\x9f-f6z0vd1J(\\xc3\\xa2b\\xc4\\x9e\\xc4\\xb0\\xe2\\x80\\x9d\\xc3\\xa2m2e\\xe2\\x80\\x9c7m0tI\\nikVPwv'tzTZp\\xc4\\xb0hwUHRZ\\xc3\\x9c/WY/L\\xc4\\xb1nfY k\\xc4\\xb0ZH\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Modeli Eğitmek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "Bu noktada problem standart bir sınıflandırma problemi olarak ele alınabilir. Önceki RNN durumu ve bu adımdaki girdi göz önüne alındığında, bir sonraki karakterin sınıfı tahmin edilmektedir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Optimizer ve Loss Fonksiyonu Eklemek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "Standart `tf.keras.losses.sparse_categorical_crossentropy` loss fonksiyonu, bu örnekte kullanmak için uygundur.\n",
        "\n",
        "Modelimiz logit döndürdüğünden, \"from_logits\" parametresini ayarlamamız gerekir.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HrXTACTdzY-",
        "outputId": "2d62a7c6-d0b2-41f6-d95c-fed5014ec301"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 91)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.511179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "Yeni başlatılan bir model kendinden çok da emin olmamalıdır, çıktı logitlerinin tümü benzer büyüklüklere sahip olmalıdır. Bunu doğrulamak için ortalama kaybın üstel değerinin yaklaşık olarak kelime boyutuna eşit olduğunu kontrol edebiliriz. Çok daha yüksek bir kayıp, modelin yanlış cevaplarından emin olduğu ve kötü bir şekilde başlatıldığı anlamına gelir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAJfS5YoFiHf",
        "outputId": "fc6c09a3-df61-48dc-a783-80c2c20885de"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.029076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "`tf.keras.Model.compile` fonksiyonunu kullanarak eğitim prosedürünü yapılandıralım. Varsayılan argümanlarla ve loss fonksiyonuyla `tf.keras.optimizers.Adam` kullanabiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Kontrol Noktalarını Yapılandırmak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Eğitim sırasında kontrol noktalarının kaydedildiğinden emin olmak için bir \"tf.keras.callbacks.ModelCheckpoint\" kullanabiliriz:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "#Kontrol noktalarının kaydedileceği konum\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "#Kontrol noktalarının adlandırılması\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Eğitimi Gerçekleştirmek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Metin Üretmek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "Bu modelle metin oluşturmanın en basit yolu, onu bir döngüde çalıştırmak ve çalıştırırken modelin dahili durumunu takip etmektir.\n",
        "\n",
        "![](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "Aşağıdaki sınıf tek adımlı tahmin yapmamıza olanak sağlamaktadır:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    #\"[UNK]\"'lerin oluşmasınının engellenmesi\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        \n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        \n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    #Stringlerin token ID'lere dönüşmesi\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    #Modelin çalıştırılması\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    #Yalnızca son tahmini kullanıyoruz\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    #Token ID'si oluşturmak için çıktı logitlerini örnekliyoruz\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    #Token ID'lerin karaktere dönüştürülmesi\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    #Karakterin ve model durumunun döndürülmesi\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Metin oluşturmak için bir döngüde çalıştıralım. Oluşturulan metne baktığınızda, modelin ne zaman büyük harf kullanacağını, paragraflar oluşturacağını, noktalama işareti kullanacağını bildiğini görebiliriz. Az sayıda epoch ile henüz tutarlı cümleler kurmayı öğrenememiş olabilir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST7PSyk9t1mT",
        "outputId": "91c4c204-dd90-4bfc-c9fb-4d34c0954a5e"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "\n",
        "seed = input(\"Başlangıç dizesi: \")\n",
        "length = int(input(\"Karakter sayısı: \"))\n",
        "\n",
        "next_char = tf.constant([seed])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(length):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Başlangıç dizesi: Hapishanede yaşarken hayat çok daha ilgi çekiciydi.\n",
            "Karakter sayısı: 200\n",
            "Hapishanede yaşarken hayat çok daha ilgi çekiciydi. Kocam çok için başına bana varik bana iteliyor! İşe kesece bu kadarımı bağışlayam da olur. Sancaklar, borazanlar, askerler yanı başımdan geçip gittir soynuma beni sevdikleri gibi severlerdi! Sonra bi \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.8362057209014893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "Sonuçları iyileştirmek için yapabileceğimiz en kolay şey, onu daha uzun süre eğitmektir (`EPOCHS = 10` deneyebiliriz).\n",
        "\n",
        "Ayrıca farklı bir başlangıç dizisi deneyebilir, modelin doğruluğunu artırmak için başka bir RNN katmanı ekleyebilir veya daha fazla veya daha az rastgele tahminler oluşturmak için temperature parametresini ayarlayabiliriz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Modeli Kaydetmek\n",
        "\n",
        "Bu model kolayca kaydedilebilir ve geri yüklenebilir, bu da onu `tf.saved_model` kabul edilen her yerde kullanmamıza olanak tanır."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Grk32H_CzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9898ef75-8c5a-4262-bb5e-616d13231d27"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'kadin_tirad_generator')\n",
        "one_step_reloaded = tf.saved_model.load('kadin_tirad_generator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fe573325350>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fe573325350>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: kadin_tirad_generator/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: kadin_tirad_generator/assets\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
